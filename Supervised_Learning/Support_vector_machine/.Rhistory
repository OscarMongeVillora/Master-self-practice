x = matrix(rnorm(40), nrow=20, ncol=2)
rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
set.seed(10111)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
class(y)
y
x
set.seed(123)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(NULL, data = NULL, kernel = "NULL", cost = 10, scale = FALSE)
print(svmfit)
set.seed(123)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
set.seed(10111)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
set.seed(10111)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
set.seed(10111)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
head(dat)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y~., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
plot(dat$X2,dat$X1, col = y + 3, pch = 19)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
svmfit$index
svmfit$coefs
svmfit$index
x[svmfit$index,]
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
xgrid
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta0
beta
abline(beta0 / beta[2], -beta[1] / beta[2])
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
?svm
rm(list=ls())
rm(list=ls())
data(iris)
iris
head(iris)
iris$Species
ntrain <- round(n*0.75)  # 75% for training set
n <- nrow(iris)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
ntrain
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
tindex <- sample(n, ntrain)   # Create a random index
train_iris <- iris[tindex,]   # Create training set
train_iris
test_iris <- iris[-tindex,]   # Create test set
head(train-train_iris)
head(train_iris)
svmfit <- svm(Species~., data=train_iris,
method="C-classification", kernal="radial", cost=10)
summary(svmfit)
svmfit$SV
summary(svmfit)
nrow(svmfit$SV)
svmfit <- svm(Species~., data=train_iris,
method="C-classification", kernel="radial", cost=10)
summary(svmfit)
svmfit$SV
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length,
slice=list(Sepal.Width=3, Sepal.Length=5))
mean(Sepal.Width)
mean(iris$Sepal.Width)
mean(iris$Sepal.Length)
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length,
slice=list(Sepal.Width=mean(iris$Sepal.Width), Sepal.Length=mean(iris$Sepal.Length)))
# hacemos la prediccion y entendemos la matriz de confusion
prediction <- predict(svmfit, test_iris)
prediction
conf_matrix <- table(prediction, test_iris$Species)
conf_matrix
diag(conf_matrix)
sum(diag(conf_matrix))/sum(conf_matrix)
rm(list=ls())
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
rm(list=ls())
data(iris)
n <- nrow(iris)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
train_iris <- iris[tindex,]   # Create training set
test_iris <- iris[-tindex,]   # Create test set
svmfit <- svm(Species~., data=train_iris,
method="C-classification", kernal="radial", cost=10)
summary(svmfit)
svmfit$SV
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
# hacemos la prediccion y entendemos la matriz de confusion
prediction <- predict(svmfit, test_iris)
conf_matrix <- table(test_iris$Species, prediction)
conf_matrix
load("C:/Users/Henry N/Google Drive/henavarr@math.uc3m.es 2018-07-10 23_17/Clases escuelas/KSchool/my_course/Supervised_Learning/Support_vector_machine/ESL.mixture.rda")
ESL.mixture
rm(list=ls())
#----------------- SVM lineal ---------------------#
#----------------- Pregunta 1 ---------------------#
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
rm(list=ls())
data(iris)
n <- nrow(iris)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
train_iris <- iris[tindex,]   # Create training set
test_iris <- iris[-tindex,]   # Create test set
svmfit <- svm(Species~., data=train_iris,
method="C-classification", kernal="radial", cost=10)
summary(svmfit)
svmfit$SV
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
# hacemos la prediccion y entendemos la matriz de confusion
prediction <- predict(svmfit, test_iris)
conf_matrix <- table(test_iris$Species, prediction)
conf_matrix
?svm
rm(list=ls())
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] + 1
x = matrix(rnorm(40), 20, 2)
x
y
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
y
y+3
plot(x, col = C("red","blue"), pch = 19)
plot(x, col = factor(C("red","blue")), pch = 19)
plot(x, col = 1:10, pch = 19)
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
?svm
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", scale = FALSE)
print(svmfit)
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
rm(list=ls())
#----------------- SVM lineal ---------------------#
#----------------- Pregunta 1 ---------------------#
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
#----------------- Pregunta 2 ---------------------#
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", scale = FALSE)
print(svmfit)
#----------------- Pregunta 3 ---------------------#
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
rm(list=ls())
#----------------- SVM lineal ---------------------#
#----------------- Pregunta 1 ---------------------#
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
#----------------- Pregunta 2 ---------------------#
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
#----------------- Pregunta 3 ---------------------#
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
svmfit$coefs
svmfit$index
t(svmfit$coefs)%*%x[svmfit$index,]
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta
beta0 = svmfit$rho
svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta
abline(-beta0 / beta[2], -beta[1] / beta[2])
abline(beta0 / beta[2], -beta[1] / beta[2])
beta0
svmfit$rho
#rho es el intercept (del plano) negativo (ver ayuda), por tanto le cambiamos el signo
beta0 = -svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(-beta0 / beta[2], -beta[1] / beta[2])
abline((1-beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(-beta0 / beta[2], -beta[1] / beta[2])
abline((1-beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline(-(1+beta0) / beta[2], -beta[1] / beta[2], lty = 2)
rm(list=ls())
data(iris)
n <- nrow(iris)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
train_iris <- iris[tindex,]   # Create training set
test_iris <- iris[-tindex,]   # Create test set
svmfit <- svm(Species~., data=train_iris,
method="C-classification", kernal="radial", cost=10)
summary(svmfit)
svmfit$SV
svmfit$index
#slice mantiene las dimensiones constantes para los valores indicados
plot(svmfit, train_iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))
# hacemos la prediccion y entendemos la matriz de confusion
prediction <- predict(svmfit, test_iris)
conf_matrix <- table(test_iris$Species, prediction)
conf_matrix
rm(list=ls())
#----------------- SVM lineal ---------------------#
#----------------- Pregunta 1 ---------------------#
set.seed(10111)
x = matrix(rnorm(40), nrow=20, ncol=2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(dat$X2,dat$X1, col = y + 3, pch = 19)
#----------------- Pregunta 2 ---------------------#
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y~., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
svmfit$index
svmfit$coef0
svmfit$coefs
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
svmfit$coe
svmfit$coefs
x[svmfit$index,]
rm(list=ls())
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
rm(list=ls())
#----------------- SVM lineal ---------------------#
#----------------- Pregunta 1 ---------------------#
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
#----------------- Pregunta 2 ---------------------#
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
#----------------- Pregunta 3 ---------------------#
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
#----------------- Pregunta 4 ---------------------#
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
#rho es el intercept (del plano) negativo (ver ayuda), por tanto le cambiamos el signo
beta0 = -svmfit$rho
#----------------- Pregunta 5 ---------------------#
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(-beta0 / beta[2], -beta[1] / beta[2])
abline((1-beta0) / beta[2], -beta[1] / beta[2], lty = 2)
abline(-(1+beta0) / beta[2], -beta[1] / beta[2], lty = 2)
#----------------- SVM no lineal ---------------------#
#----------------- Pregunta 1 ---------------------#
rm(list=ls())
data(iris)
n <- nrow(iris)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
set.seed(314)    # Set seed for reproducible results
tindex <- sample(n, ntrain)   # Create a random index
train_iris <- iris[tindex,]   # Create training set
test_iris <- iris[-tindex,]   # Create test set
svmfit <- svm(Species~., data=train_iris,
method="C-classification", kernal="radial", cost=10)
summary(svmfit)
if (!require("e1071")){install.packages("e1071",verbose = F) ; library("e1071")}
print("All packages and functions have been installed or loaded...")
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x
y
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
dat
plot(x, col = y + 3, pch = 19)
plot(x, col = y + 3, pch = 19)
plot(x, pch = 19)
plot(x, col = y + 3, pch = 19)
?grange
points(x[svmfit$index,], pch = 5, cex = 2)
