train=sample(1:nrow(carseats), 250) #tomamos 250 valores de 400
#Creamos el decision tree solo con los indices de train
tree.carseats = tree(High~.-Sales, carseats, subset=train)
#visualizamos
plot(tree.carseats)
text(tree.carseats, pretty=0)
# Se ve un poco distinto debido al conjunto de datos ligeramente diferente.
tree.pred = predict(tree.carseats, carseats[-train,], type="class")
#matriz de confusion solo para datos de test
conf_matrix<-with(carseats[-train,], table(pred=tree.pred,actual=High))
#---- Pregunta 5: Cost-complexity pruning ------#
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
#size: numero de nodos terminales
#dev: numero de valores mal clasificados (esto es porque
#hemos cambiado la funcion de pruning)
#k: parametro de pruning mas info: en http://mlwiki.org/index.php/Cost-Complexity_Pruning
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$size,cv.carseats$dev,type="l")
plot(cv.carseats$size,cv.carseats$dev)
plot(cv.carseats$size,cv.carseats$dev,type="l")
plot(cv.carseats$size,cv.carseats$dev,type="b")
cv.carseats$size
sort(cv.carseats$size)
order(cv.carseats$size)
cv.carseats$size[order(cv.carseats$size)]
sort(cv.carseats$size)
sort(cv.carseats$dev)
index_order<-order(cv.carseats$size)
cv.carseats$size[index_order]
cv.carseats$dev[index_order]
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty=0)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
(75 + 36) / 150
#--------------------------------------------------------------------------------------------------- #
#  ____    _                    _           ____                    __   _   _
# / ___|  | |_    __ _   _ __  | |_   _    |  _ \   _ __    ___    / _| (_) | |   ___
# \___ \  | __|  / _` | | '__| | __| (_)   | |_) | | '__|  / _ \  | |_  | | | |  / _ \
#  ___) | | |_  | (_| | | |    | |_   _    |  __/  | |    | (_) | |  _| | | | | |  __/
# |____/   \__|  \__,_| |_|     \__| (_)   |_|     |_|     \___/  |_|   |_| |_|  \___|
#--------------------------------------------------------------------------------------------------- #
if (!require("ISLR")){install.packages("ISLR",verbose = F) ; library("ISLR")}
if (!require("tree")){install.packages("tree",verbose = F) ; library("tree")}
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
names(carseats)
colnames(carseats)
carseats
carseats
?Carseats
head(carseats)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales)
#Clase de cada variable
sapply(carseats,class)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(density(carseats$Sales))
lines(carseats$Sales)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(carseats$Sales)
?density
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(density(carseats$Sales))
plot(carseats$Sales)
plot(carseats)
plot(carseats)
plot(carseats)
head(carseats)
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
carseats
ifelse(carseats$Sales<=8, "No", "Yes")
qqnorm?
?qqnorm
?qqnorm
qqnorm(carseats$Sales)
qqlines(carseats$Sales)
qqline(carseats$Sales)
#Borramos los datos
rm(list=ls())
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
names(carseats)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
#creamos un decision tree para todo el dataset menos Sales
tree.carseats = tree(High~.-Sales, data=carseats)
#vemos los resultados del arbol
summary(tree.carseats)
#Borramos los datos
rm(list=ls())
#Borramos los datos
rm(list=ls())
#Borramos los datos
rm(list=ls())
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
names(carseats)
colnames(carseats)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(density(carseats$Sales))
#Clase de cada variable
sapply(carseats,class)
plot(carseats)
qqline(carseats$Sales)
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
#creamos un decision tree para todo el dataset menos Sales
tree.carseats = tree(High~.-Sales, data=carseats)
#vemos los resultados del arbol
summary(tree.carseats)
qqline(carseats$Sales)
qqline(carseats$Sales)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(density(carseats$Sales))
#vemos los resultados del arbol
summary(tree.carseats)
plot(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
?carseats
??carseats
?Carseats
plot(tree.carseats)
text(tree.carseats, pretty = 0)
text(tree.carseats, pretty = 10)
text(tree.carseats, pretty = 100)
text(tree.carseats, pretty = 15)
text(tree.carseats, pretty = 2)
plot(tree.carseats)
text(tree.carseats, pretty = 2)
plot(tree.carseats)
text(tree.carseats, pretty = 10)
plot(tree.carseats)
text(tree.carseats, pretty = 2)
plot(tree.carseats)
text(tree.carseats, pretty = 4)
train=sample(1:nrow(carseats), 250) #tomamos 250 valores de 400
#Creamos el decision tree solo con los indices de train
tree.carseats = tree(High~.-Sales, carseats, subset=train)
#visualizamos
plot(tree.carseats)
text(tree.carseats, pretty=0)
train=sample(1:nrow(carseats), 250) #tomamos 250 valores de 400
#Creamos el decision tree solo con los indices de train
tree.carseats = tree(High~.-Sales, carseats, subset=train)
#visualizamos
plot(tree.carseats)
text(tree.carseats, pretty=0)
train=sample(1:nrow(carseats), 250) #tomamos 250 valores de 400
#Creamos el decision tree solo con los indices de train
tree.carseats = tree(High~.-Sales, carseats, subset=train)
#visualizamos
plot(tree.carseats)
text(tree.carseats, pretty=0)
# Se ve un poco distinto debido al conjunto de datos ligeramente diferente.
tree.pred = predict(tree.carseats, carseats[-train,], type="class")
tree.pred
#matriz de confusion solo para datos de test
with(carseats[-train,], table(tree.pred, High))
#matriz de confusion solo para datos de test
with(carseats[train,], table(tree.pred, High))
# Se ve un poco distinto debido al conjunto de datos ligeramente diferente.
tree.pred = predict(tree.carseats, carseats[train,], type="class")
#matriz de confusion solo para datos de test
with(carseats[train,], table(tree.pred, High))
# Vemos que el predictor mas fuerte es la ubicación en el estante.
# Los asientos con “buenos” estantes tienen una mayor probabilidad de comprarse.
# El lado del arbol que tiene en cuenta las estanterias medianas / malas es considerablemente mas complejo,
# y si bien el precio sigue siendo importante, algunas otras variables como la publicidad también tienen poder
# de prediccion. En general, podríamos hacerlo un poco mejor podando el arbol.
# Usaremos la validación cruzada para determinar el numero ideal de nodos
# terminales en función del los errores de clasificación en el conjunto de prueba.
tree.pred = predict(tree.carseats, carseats, type="class")
#matriz de confusion solo para datos de test
with(carseats, table(tree.pred, High))
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
cv.carseats
cv.carseats
?cv.carseats
?cv
?cv.tree
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
prune.misclass(tree.carseats)
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
prune.misclass(tree.carseats)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
plot(prune.carseats)
plot(prune.carseats)
text(prune.carseats, pretty=0)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 7)
plot(prune.carseats)
text(prune.carseats, pretty=0)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 6)
plot(prune.carseats)
text(prune.carseats, pretty=0)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty=0)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 7)
plot(prune.carseats)
text(prune.carseats, pretty=0)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty=0)
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$size,cv.carseats$dev,type="b")
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
?prune.misclass
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
#Creamos el decision tree solo con los indices de train
tree.carseats = tree(High~.-Sales, carseats, subset=train)
# Se ve un poco distinto debido al conjunto de datos ligeramente diferente.
tree.pred = predict(tree.carseats, carseats[-train,], type="class")
#matriz de confusion solo para datos de test
with(carseats[-train,], table(tree.pred, High))
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
#Borramos los datos
rm(list=ls())
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
#Borramos los datos
rm(list=ls())
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
names(carseats)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(density(carseats$Sales))
#Clase de cada variable
sapply(carseats,class)
qqplot(carseats)
qqplot(carseats$Sales)
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
qqplot(carseats$High)
qqpnorm(carseats$Sales)
qqnorm(carseats$Sales)
qqline(carseats$Sales)
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
#creamos un decision tree para todo el dataset menos Sales
tree.carseats = tree(High~.-Sales, data=carseats)
#vemos los resultados del arbol
summary(tree.carseats)
#creamos un decision tree para todo el dataset menos Sales
tree.carseats = tree(High~.-Sales, data=carseats)
#vemos los resultados del arbol
summary(tree.carseats)
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
#---- Pregunta 1: Previsualizacion de los datos ------#
hist(carseats$Sales, prob = T)
lines(density(carseats$Sales))
#Clase de cada variable
sapply(carseats,class)
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
#creamos un decision tree para todo el dataset menos Sales
tree.carseats = tree(High~.-Sales, data=carseats)
#vemos los resultados del arbol
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 4)
tree.carseats
?tree
tree.carseats
tree.carseats
tree.carseats
plot(tree.carseats)
text(tree.carseats, pretty = 4)
carseats
table(carseats)
df(carseats)
data.frame(carseats)
unique(carseats$ShelveLoc)
tree.carseats
carseats[carseats$ShelveLoc == "Good"]
a <- carseats[carseats$ShelveLoc == "Good",]
length(a)
size(a)
dim(a)
summary(tree.carseats)
#matriz de confusion
with(carseats, table(tree.pred, High))
# Vemos que el predictor mas fuerte es la ubicación en el estante.
# Los asientos con “buenos” estantes tienen una mayor probabilidad de comprarse.
# El lado del arbol que tiene en cuenta las estanterias medianas / malas es considerablemente mas complejo,
# y si bien el precio sigue siendo importante, algunas otras variables como la publicidad también tienen poder
# de prediccion. En general, podríamos hacerlo un poco mejor podando el arbol.
# Usaremos la validación cruzada para determinar el numero ideal de nodos
# terminales en función del los errores de clasificación en el conjunto de prueba.
tree.pred = predict(tree.carseats, carseats, type="class")
#matriz de confusion
with(carseats, table(tree.pred, High))
# Vemos que el predictor mas fuerte es la ubicación en el estante.
# Los asientos con “buenos” estantes tienen una mayor probabilidad de comprarse.
# El lado del arbol que tiene en cuenta las estanterias medianas / malas es considerablemente mas complejo,
# y si bien el precio sigue siendo importante, algunas otras variables como la publicidad también tienen poder
# de prediccion. En general, podríamos hacerlo un poco mejor podando el arbol.
# Usaremos la validación cruzada para determinar el numero ideal de nodos
# terminales en función del los errores de clasificación en el conjunto de prueba.
tree.pred = predict(tree.carseats, carseats, type="class")
#matriz de confusion
with(carseats, table(tree.pred, High))
#Borramos los datos
rm(list=ls())
#Lectura de los datos
#data(package="ISLR")
carseats<-Carseats
#creamos una variable dicotomica para sales
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
#creamos un decision tree para todo el dataset menos Sales
tree.carseats = tree(High~.-Sales, data=carseats)
#vemos los resultados del arbol
summary(tree.carseats)
# Vemos que el predictor mas fuerte es la ubicación en el estante.
# Los asientos con “buenos” estantes tienen una mayor probabilidad de comprarse.
# El lado del arbol que tiene en cuenta las estanterias medianas / malas es considerablemente mas complejo,
# y si bien el precio sigue siendo importante, algunas otras variables como la publicidad también tienen poder
# de prediccion. En general, podríamos hacerlo un poco mejor podando el arbol.
# Usaremos la validación cruzada para determinar el numero ideal de nodos
# terminales en función del los errores de clasificación en el conjunto de prueba.
tree.pred = predict(tree.carseats, carseats, type="class")
#matriz de confusion
with(carseats, table(tree.pred, High))
set.seed(3) #para que puedan obtener todos el mismo valor
train=sample(1:nrow(carseats), 250) #tomamos 250 valores de 400
#Creamos el decision tree solo con los indices de train
tree.carseats = tree(High~.-Sales, carseats, subset=train)
# Se ve un poco distinto debido al conjunto de datos ligeramente diferente.
tree.pred = predict(tree.carseats, carseats[-train,], type="class")
#matriz de confusion solo para datos de test
with(carseats[-train,], table(tree.pred, High))
# Se ve un poco distinto debido al conjunto de datos ligeramente diferente.
tree.pred = predict(tree.carseats, carseats[train,], type="class")
#matriz de confusion solo para datos de test
with(carseats[train,], table(tree.pred, High))
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 30)
cv.carseats
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 200)
cv.carseats
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
cv.carseats
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$size,cv.carseats$dev,type="b")
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
plot(cv.carseats$size,cv.carseats$dev,type="b")
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 6)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
plot(cv.carseats$size,cv.carseats$dev,type="b")
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
with(carseats[train,], table(tree.pred, High))
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[train,], type="class")
with(carseats[train,], table(tree.pred, High))
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 6)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[train,], type="class")
with(carseats[train,], table(tree.pred, High))
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 6)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
(75 + 36) / 150
# Vamos a usar la validación cruzada para podar el arbol de manera optima.
# prune.misclass usa el error de clasificación erronea como base para hacer la poda.
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass,K = 10)
plot(cv.carseats$size,cv.carseats$dev,type="b")
#seleccionamos finalmente el numero de nodos terminales
prune.carseats = prune.misclass(tree.carseats, best = 8)
#clasificamos los valores de prediccion
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
